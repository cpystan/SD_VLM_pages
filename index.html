<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models">
  <meta name="keywords" content="MLLM, Spatial Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models</title>
  <style>
    .bold {
        font-weight: bold;
    }
    .highlight {
      background-color: #CCF2FF; /* 浅蓝色背景 */
      }
    table {
        width: 100%;
    }
</style>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">NeurIPS 2025</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Pingyi Chen</a><sup>1,2,3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Yujing Lou</a><sup>3,4</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Shen Cao</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Jinhui Guo</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Lubin Fan</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <br>
            <span class="author-block">Yue Wu<sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Lin Yang<sup>2</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Lizhuang Ma</a><sup>4</sup>&nbsp&nbsp&nbsp</span>
            <span class="author-block">Jieping Ye<sup>3</sup>,&nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University &nbsp&nbsp&nbsp</span>
            <!-- <span class="author-block"><sup>2</sup>Alibaba Cloud Computing &nbsp&nbsp&nbsp</span> -->
            <span class="author-block"><sup>2</sup>Westlake University &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>3</sup>Alibaba Cloud &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University &nbsp&nbsp&nbsp</span>
          </div>

          <!-- <div class="is-size-6 publication-authors"> -->
            <!-- <span class="author-block"><sup>*</sup>This work was done when Pingyi Chen was an intern at Alibaba Cloud.</span> -->
          <!-- </div> -->

          <!-- <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&dagger;</sup>Equal contributions.</span>
          </div> -->


          <div class="column has-text-centered">
<!--            <div class="publication-links">-->
<!--               PDF Link.-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.17664"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/cpystan/MSMU"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/cpystan/MSMU"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">&#129303</span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
 <div class="container">
   <div class="columns is-centered">
     <div class="column">
        <div class="column has-text-centered"> 
         <img src="static/images/vis_qa.png" alt="Teaser image." style="width: 80%; max-width: 1280px;">
         <h2 class="subtitle has-text-justified" style="font-size: 14px;">
          <b>SD-VLM</b> Our work (1) proposes Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduces a plug-and-play depth positional encoding method strengthening VLMs’ spatial awareness.
         </h2>
     </div>
       </div>
   </div>
 </div>
</section>

<br>
<br>

<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
  While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under-explored, due to the deficiency of 2D images' spatial representation ability. In this paper, we analyze the problem hindering VLMs' spatial understanding abilities and propose SD-VLM, a novel framework that significantly  enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs' spatial awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples.  We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. We will release MSMU dataset and SD-VLM to facilitate future research in quantitative spatial measuring and understanding. 
          </p>
        </div>
      </div>
    </div>
  </div>
    <!-- / Abstract. -->
  </div>
</section>

<br>
<br>


<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <img src="./static/images/pipeline.png" />
        <figcaption>Figure: Pipeline.</figcaption>
        <div class="content has-text-justified">
          <p>
Starting from 3D scene point clouds, we first collect the spatial information (e.g., locations, sizes, relative distances) of objects in the scene to construct a scene graph. Next, we rasterize 3D instances into 2D images and establish a 3D-to-2D mapping, which enables transferring spatial annotations to images. We also perform filtering on both images and objects to ensure the quality of the QA pairs. Finally, we design human-verified QA templates and employ LLM collaboration to generate a rich set of QA pairs.        </p>
        <p>
  <b>(step 1) Building Scene Graph.</b> Given a 3D point cloud of a scene, we first construct a scene graph (stored as a JSON file) to systematically organize all annotations and metadata. This graph includes the object categorization and corresponding 3D spatial localization data which provides bounding boxes for each object, defined by centroid coordinates and dimensional parameters.
         <p>
<b>(step 2) Rasterize 3D instances to 2D images.</b> We rasterize 3D instances onto images as masks  using official tools. This process bridge an object in the 3D scene and 2D image plane, making transferring spatial annotations from 3D scene graph to each image feasible. 
          <p>
<b>(step 3) Image Filtering and Object Selection.</b> We first sparsely sample the RGB images to reduce redundancy. After that, we carefully select objects in each image, which is guided by three principal criteria: 
(1) Prevalence and functionality.
We focus on objects demonstrating clear functional purposes which are commonly encountered in indoor environments. Architectural components (e.g., walls, ceilings) are excluded due to their limited interactive potential. (2) Instance visibility. Objects that are partially occluded (e.g., a chair mostly hidden behind a table), truncated by image borders (e.g., only a corner of a table is visible), or too small to annotate reliably (e.g., distant objects occupying fewer than 50 pixels) are excluded from our dataset.
(3) Semantic disambiguation.
Addressing linguistic ambiguity is important before generating annotations. For example, tables which exist in one image may vary in color or texture but are all labeled as "table", which brings noisy correspondence and finally misleads VLMs. To mitigate this issue, we resort to Qwen2.5-VL to re-label these objects with more detailed descriptions, such as "the white table" or "the wooden table". Finally, we filter out non-informative images that have no valid objects.
  <p>
<b>(step 4) Templated-based Generation.</b> We carefully design a set of templates based on the task definitions which include various placeholders. For instance, one template for measuring the size of a single target object is structured as follows:
``Q: What is the size of [object A]. A: The size of [object A] is [Length]x[Width]x[Height].''}
  For each image, we enumerate the selected objects and replace these placeholders with the corresponding object labels or spatial annotations. In tasks involving two or more target objects, we also meticulously craft instructions that incorporate all relevant object labels and spatial information.
  <p>
 <b>(step 5) Eliciting Reasoning Path with LLM collaboration.</b>   We improve the quantitative spatial ability of VLMs by eliciting
reasoning paths with reference
objects, we augment the QA pairs with CoT reasoning rationale via LLM collaboration. Specifically, we randomly select one object as the reference object and combine its spatial annotations along with the image as inputs to the advanced VLM, Qwen2.5-VL. The VLM is then prompted to construct a reasoning path that leverages the reference object to infer the spatial properties of another object within the image. Subsequently, we utilize a large language model, DeepSeek-V3, to assess and filter the CoT pairs by evaluating the factual consistency and logical coherence.       
          <p>
We employ this data generation pipeline to construct VQA pairs from ScanNet and ScanNet++. The resulting MSMU dataset contains 2K scenes, 25K images, 75K objects, 700K QA pairs, and 2.5M numerical values, covering a wide range of quantitative spatial tasks. Besides, the CoT augmented group, named MSMU-CoT, consists of 10K quantitative spatial reasoning QA pairs.        </p>
        </p>
        </div>
      </div>
    </div>
  </div>
</section>


<br>
<br>

<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model</h2>
        <img src="./static/images/framework.png" />
        <figcaption>Figure: Framework.</figcaption>
        <div class="content has-text-justified">
          <p>
We introduce the depth positional encoding (DPE), which can encode the depth maps into depth positional embeddings, allowing for a straightforward combination through addition.        
          We utilize sine and cosine functions of varying frequencies to generate the depth positional embeddings        

          <p>
The model consists of a vision encoder to encode image features, a depth encoding module to incorporate depth information, and a large language model to process sequences of tokens. When depth maps are not accessible during inference, we employ an external depth estimation model to generate the depth map. This allows our model to adapt to various datasets and scenarios effectively.          </p>

        </div>
      </div>
    </div>
  </div>
</section>
  
<br>
  <br>



  <section class="hero teaser">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Results</h2>

        <div class="content has-text-justified">
          <div class="table-container">
            <table style="width: 100%;">
  <thead>
    <tr>
      <th rowspan="2">Model</th>
      <th colspan="8">Task</th>
      <th rowspan="2">Average</th>
    </tr>
    <tr>
      <th>Existence</th><th>Object<br>Counting</th><th>Scale<br>Est.</th>
      <th>Grounding</th><th>Relative<br>Position</th><th>Absolute<br>Distance</th>
      <th>Scale<br>Comparison</th><th>Ref. Object<br>Est.</th>
    </tr>
  </thead>

  <tbody>
    <tr><td colspan="10" style="background:#e8e8e8;text-align:center;"><strong>Large Language Models (LLMs): Text only</strong></td></tr>
    <tr><td>GPT-4-Turbo</td><td>12.76</td><td>5.21</td><td>13.51</td><td>12.64</td><td>24.84</td><td>7.50</td><td>36.79</td><td>12.04</td><td>15.66</td></tr>
    <tr><td>Qwen2.5</td><td>4.25</td><td>0.00</td><td>0.78</td><td>13.79</td><td>0.62</td><td>0.00</td><td>16.04</td><td>1.57</td><td>4.63</td></tr>
    <tr><td>DeepSeek-V3</td><td>0.00</td><td>5.24</td><td>1.54</td><td>6.90</td><td>10.56</td><td>0.00</td><td>25.47</td><td>5.24</td><td>7.39</td></tr>

    <tr><td colspan="10" style="background:#e8e8e8;text-align:center;"><strong>Vision-Language Models (VLMs): Image + Text</strong></td></tr>
    <tr><td>GPT-4o</td><td>44.68</td><td>41.67</td><td>3.86</td><td>27.59</td><td>67.08</td><td>20.00</td><td>54.72</td><td>2.09</td><td>32.28</td></tr>
    <tr><td>Gemini-2</td><td>38.30</td><td>43.75</td><td>23.94</td><td>19.54</td><td>54.66</td><td>12.50</td><td>69.81</td><td>18.85</td><td>35.17</td></tr>
    <tr><td>Qwen2.5-VL-72B</td><td>59.57</td><td>35.42</td><td>1.54</td><td>13.79</td><td>57.76</td><td>2.50</td><td>66.04</td><td>9.95</td><td>30.82</td></tr>
    <tr><td>Qwen2.5-VL-32B</td><td>29.79</td><td>41.67</td><td>10.81</td><td>18.39</td><td>60.25</td><td>2.50</td><td>46.23</td><td>10.99</td><td>27.59</td></tr>
    <tr><td>Qwen2.5-VL-7B</td><td>12.76</td><td>4.17</td><td>0.00</td><td>1.15</td><td>1.24</td><td>0.00</td><td>5.66</td><td>0.52</td><td>3.19</td></tr>
    <tr><td>Intern-VL3-78B</td><td>47.62</td><td>42.71</td><td>6.47</td><td>26.32</td><td>56.94</td><td>13.33</td><td>64.10</td><td>16.46</td><td>33.63</td></tr>
    <tr><td>Intern-VL3-8B</td><td>36.17</td><td>41.67</td><td>4.63</td><td>18.39</td><td>60.25</td><td>2.50</td><td>49.06</td><td>8.38</td><td>28.54</td></tr>
    <tr><td>LLaVA-1.5-7B</td><td>1.54</td><td>36.46</td><td>5.02</td><td>20.69</td><td>42.86</td><td>5.00</td><td>38.68</td><td>0.52</td><td>19.45</td></tr>

    <tr><td colspan="10" style="background:#e8e8e8;text-align:center;"><strong>Depth-encoded VLMs: Image + Depth + Text</strong></td></tr>
    <tr><td>SpatialBot</td><td>10.64</td><td>46.88</td><td>15.83</td><td>28.74</td><td>66.46</td><td>5.00</td><td>50.94</td><td>8.90</td><td>29.17</td></tr>
    <tr><td>SpatialRGPT</td><td>10.64</td><td>36.46</td><td>20.08</td><td>17.24</td><td>60.25</td><td>15.00</td><td>62.26</td><td>9.95</td><td>28.98</td></tr>

    <tr style="background:#dcdcdc;"><td>Ours</td>
      <td class="best"><strong>87.23</strong></td><td class="best"><strong>47.92</strong></td><td>51.35</td><td>42.53</td><td class="best"><strong>75.16</strong></td><td>40.00</td><td>55.66</td><td>46.07</td><td>56.31</td></tr>
    <tr style="background:#dcdcdc;"><td>Ours w/ MSMU-CoT</td>
      <td class="best"><strong>87.23</strong></td><td>42.71</td><td class="best"><strong>51.74</strong></td><td class="best"><strong>49.43</strong></td><td>73.29</td><td class="best"><strong>50.00</strong></td><td class="best"><strong>69.81</strong></td><td class="best"><strong>49.32</strong></td><td><strong>59.19</strong></td></tr>
  </tbody>
</table>
                        </div>
        </div>
      </div>
    </div>
  </div>
</section>
  


<br>
<br>


<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Visualization</h2>
        <img src="./static/images/11.png" />
        <figcaption>Figure: Visualization of responses from different models on MSMU-Bench.</figcaption>

        <div class="content has-text-justified">

        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
In this work, we identified a critical gap in the ability of Vision-Language Models (VLMs) to perform quantitative spatial reasoning. To address this, we developed MSMU, a large-scale dataset comprising 700K QA pairs and 2.5M numerical physical annotations derived from real 3D scenes, designed to provide precise metric supervision for enhancing VLMs' spatial reasoning capabilities. We introduced a simple but effective depth positional encoding module that integrates the third dimension information into the VLM frameworks, effectively upgrading the model's spatial awareness from 2D to 3D. This innovation was shown to significantly enhance spatial reasoning abilities, outperforming both RGB-only VLMs and depth-encoded VLMs. We anticipate that our contributions will pave the way for further advancements in VLMs' spatial reasoning capabilities, enabling more effective operation in real-world environments.       

          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<br>
<br>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{chen2025sdvlm,
    author = {Pingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin Yang, Lizhuang Ma, Jieping Ye},
    title = {SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models},
    booktitle = {NeurIPS},
    year = {2025}
}</code></pre>
  </div>
</section>



</body>
</html>
