<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models">
  <meta name="keywords" content="MLLM, Spatial Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models</title>
  <style>
    .bold {
        font-weight: bold;
    }
    .highlight {
      background-color: #CCF2FF; /* 浅蓝色背景 */
      }
    table {
        width: 100%;
    }
</style>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">NeurIPS 2025</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Pingyi Chen</a><sup>1,2</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Yujing Lou</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Shen Cao</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Jinhui Guo</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Lubin Fan</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <br>
            <span class="author-block">Yue Wu<sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Lin Yang<sup>2</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Lizhuang Ma</a><sup>4</sup>&nbsp&nbsp&nbsp</span>
            <span class="author-block">Jieping Ye<sup>3</sup>,&nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University &nbsp&nbsp&nbsp</span>
            <!-- <span class="author-block"><sup>2</sup>Alibaba Cloud Computing &nbsp&nbsp&nbsp</span> -->
            <span class="author-block"><sup>2</sup>Westlake University &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>3</sup>Alibaba Cloud &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>4</sup>Shanghai Jiaotong University &nbsp&nbsp&nbsp</span>
          </div>

          <!-- <div class="is-size-6 publication-authors"> -->
            <!-- <span class="author-block"><sup>*</sup>This work was done when Jingyu Lin was an intern at Alibaba Cloud.</span> -->
          <!-- </div> -->

          <!-- <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&dagger;</sup>Equal contributions.</span>
          </div> -->


          <div class="column has-text-centered">
<!--            <div class="publication-links">-->
<!--               PDF Link.-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.03844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Yeyuqqwx/HybridGS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/Eto63277/HybridGS/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">&#129303</span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
 <div class="container">
   <div class="columns is-centered">
     <div class="column">
         <img src="static/images/11.png" alt="Teaser image."/>
         <h2 class="subtitle has-text-justified" style="font-size: 14px;">
          <b>SD-VLM</b> Our work (1) proposes Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduces a plug-and-play depth positional encoding method strengthening VLMs’ spatial awareness.
         </h2>
     </div>
   </div>
 </div>
</section>

<br>
<br>

<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
  While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under-explored, due to the deficiency of 2D images' spatial representation ability. In this paper, we analyze the problem hindering VLMs' spatial understanding abilities and propose SD-VLM, a novel framework that significantly  enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs' spatial awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples.  We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. We will release MSMU dataset and SD-VLM to facilitate future research in quantitative spatial measuring and understanding. 
          </p>
        </div>
      </div>
    </div>
  </div>
    <!-- / Abstract. -->
  </div>
</section>

<br>
<br>


<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <img src="./static/images/pipeline.png" />
        <figcaption>Figure: Pipeline.</figcaption>
        <div class="content has-text-justified">
          <p>
Starting from 3D scene point clouds, we first collect the spatial information (e.g., locations, sizes, relative distances) of objects in the scene to construct a scene graph. Next, we rasterize 3D instances into 2D images and establish a 3D-to-2D mapping, which enables transferring spatial annotations to images. We also perform filtering on both images and objects to ensure the quality of the QA pairs. Finally, we design human-verified QA templates and employ LLM collaboration to generate a rich set of QA pairs.        </p>
        <p>
  <b>(step 1) Building Scene Graph.</b> Given a 3D point cloud of a scene, we first construct a scene graph (stored as a JSON file) to systematically organize all annotations and metadata. This graph includes the object categorization and corresponding 3D spatial localization data which provides bounding boxes for each object, defined by centroid coordinates and dimensional parameters.
         <p>
<b>(step 2) Rasterize 3D instances to 2D images.</b> We rasterize 3D instances onto images as masks  using official tools. This process bridge an object in the 3D scene and 2D image plane, making transferring spatial annotations from 3D scene graph to each image feasible. 
          <p>
<b>(step 3) Image Filtering and Object Selection.</b> We first sparsely sample the RGB images to reduce redundancy. After that, we carefully select objects in each image, which is guided by three principal criteria: 
(1) Prevalence and functionality.
We focus on objects demonstrating clear functional purposes which are commonly encountered in indoor environments. Architectural components (e.g., walls, ceilings) are excluded due to their limited interactive potential. (2) Instance visibility. Objects that are partially occluded (e.g., a chair mostly hidden behind a table), truncated by image borders (e.g., only a corner of a table is visible), or too small to annotate reliably (e.g., distant objects occupying fewer than 50 pixels) are excluded from our dataset.
(3) Semantic disambiguation.
Addressing linguistic ambiguity is important before generating annotations. For example, tables which exist in one image may vary in color or texture but are all labeled as "table", which brings noisy correspondence and finally misleads VLMs. To mitigate this issue, we resort to Qwen2.5-VL to re-label these objects with more detailed descriptions, such as "the white table" or "the wooden table". Finally, we filter out non-informative images that have no valid objects.
  <p>
<b>(step 4) Templated-based Generation.</b> We carefully design a set of templates based on the task definitions which include various placeholders. For instance, one template for measuring the size of a single target object is structured as follows:
``Q: What is the size of [object A]. A: The size of [object A] is [Length]x[Width]x[Height].''}
  For each image, we enumerate the selected objects and replace these placeholders with the corresponding object labels or spatial annotations. In tasks involving two or more target objects, we also meticulously craft instructions that incorporate all relevant object labels and spatial information.
  <p>
 <b>(step 5) Eliciting Reasoning Path with LLM collaboration.</b>   We improve the quantitative spatial ability of VLMs by eliciting
reasoning paths with reference
objects, we augment the QA pairs with CoT reasoning rationale via LLM collaboration. Specifically, we randomly select one object as the reference object and combine its spatial annotations along with the image as inputs to the advanced VLM, Qwen2.5-VL. The VLM is then prompted to construct a reasoning path that leverages the reference object to infer the spatial properties of another object within the image. Subsequently, we utilize a large language model, DeepSeek-V3, to assess and filter the CoT pairs by evaluating the factual consistency and logical coherence.       
          <p>
We employ this data generation pipeline to construct VQA pairs from ScanNet and ScanNet++. The resulting MSMU dataset contains 2K scenes, 25K images, 75K objects, 700K QA pairs, and 2.5M numerical values, covering a wide range of quantitative spatial tasks. Besides, the CoT augmented group, named MSMU-CoT, consists of 10K quantitative spatial reasoning QA pairs.        </p>
        </p>
        </div>
      </div>
    </div>
  </div>
</section>


<br>
<br>

<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model</h2>
        <img src="./static/images/framework.png" />
        <figcaption>Figure: Framework.</figcaption>
        <div class="content has-text-justified">
          <p>
We introduce the depth positional encoding (DPE), which can encode the depth maps into depth positional embeddings, allowing for a straightforward combination through addition.        
          We utilize sine and cosine functions of varying frequencies to generate the depth positional embeddings        

          <p>
The model consists of a vision encoder to encode image features, a depth encoding module to incorporate depth information, and a large language model to process sequences of tokens. When depth maps are not accessible during inference, we employ an external depth estimation model to generate the depth map. This allows our model to adapt to various datasets and scenarios effectively.          </p>

        </div>
      </div>
    </div>
  </div>
</section>
  
<br>
  <br>
 <table>
        <caption>Overall results of various models on MSMU-Bench. We report the results of LLMs, VLMs, and depth-encoded VLMs as a comprehensive comparison.</caption>
        <thead>
            <tr>
                <th class="multi-row" rowspan="2">Model</th>
                <th rowspan="1">Existence</th>
                <th rowspan="1">Object Counting</th>
                <th rowspan="1">Scale Estimation</th>
                <th class="multi-row" rowspan="2">Grounding</th>
                <th rowspan="1">Relative Position</th>
                <th rowspan="1">Absolute Distance</th>
                <th rowspan="1">Scale Comparison</th>
                <th rowspan="1">Ref. Object Estimation</th>
                <th class="multi-row" rowspan="2">Average</th>
            </tr>
            <tr>
                <th>Existence</th>
                <th>Object Counting</th>
                <th>Scale Estimation</th>
                <th>Relative Position</th>
                <th>Absolute Distance</th>
                <th>Scale Comparison</th>
                <th>Ref. Object Estimation</th>
            </tr>
        </thead>
        <tbody>
            <tr class="group-header">
                <td colspan="10">Large Language Models (LLMs): Only Text as Input</td>
            </tr>
            <tr>
                <td class="left-align">GPT-4-Turbo<sup>1</sup></td>
                <td>12.76</td>
                <td>5.21</td>
                <td>13.51</td>
                <td>12.64</td>
                <td>24.84</td>
                <td>7.50</td>
                <td>36.79</td>
                <td>12.04</td>
                <td>15.66</td>
            </tr>
            <tr>
                <td class="left-align">Qwen2.5<sup>2</sup></td>
                <td>4.25</td>
                <td>0.00</td>
                <td>0.78</td>
                <td>13.79</td>
                <td>0.62</td>
                <td>0.00</td>
                <td>16.04</td>
                <td>1.57</td>
                <td>4.63</td>
            </tr>
            <tr>
                <td class="left-align">DeepSeek-V3<sup>3</sup></td>
                <td>0.00</td>
                <td>5.24</td>
                <td>1.54</td>
                <td>6.90</td>
                <td>10.56</td>
                <td>0.00</td>
                <td>25.47</td>
                <td>5.24</td>
                <td>7.39</td>
            </tr>
            <tr class="group-header">
                <td colspan="10">Vision-Language Models (VLMs): Image + Text as Input</td>
            </tr>
            <tr>
                <td class="left-align">GPT-4o<sup>4</sup></td>
                <td>44.68</td>
                <td>41.67</td>
                <td>3.86</td>
                <td>27.59</td>
                <td>67.08</td>
                <td>20.00</td>
                <td>54.72</td>
                <td>2.09</td>
                <td>32.28</td>
            </tr>
            <tr>
                <td class="left-align">Gemini-2<sup>5</sup></td>
                <td>38.30</td>
                <td>43.75</td>
                <td>23.94</td>
                <td>19.54</td>
                <td>54.66</td>
                <td>12.50</td>
                <td class="bold">69.81</td>
                <td>18.85</td>
                <td>35.17</td>
            </tr>
            <tr>
                <td class="left-align">Qwen2.5-VL-72B<sup>2</sup></td>
                <td>59.57</td>
                <td>35.42</td>
                <td>1.54</td>
                <td>13.79</td>
                <td>57.76</td>
                <td>2.50</td>
                <td>66.04</td>
                <td>9.95</td>
                <td>30.82</td>
            </tr>
            <tr>
                <td class="left-align">Qwen2.5-VL-32B<sup>2</sup></td>
                <td>29.79</td>
                <td>41.67</td>
                <td>10.81</td>
                <td>18.39</td>
                <td>60.25</td>
                <td>2.50</td>
                <td>46.23</td>
                <td>10.99</td>
                <td>27.59</td>
            </tr>
            <tr>
                <td class="left-align">Qwen2.5-VL-7B<sup>2</sup></td>
                <td>12.76</td>
                <td>4.17</td>
                <td>0.00</td>
                <td>1.15</td>
                <td>1.24</td>
                <td>0.00</td>
                <td>5.66</td>
                <td>0.52</td>
                <td>3.19</td>
            </tr>
            <tr>
                <td class="left-align">Intern-VL3-78B<sup>6</sup></td>
                <td>47.62</td>
                <td>42.71</td>
                <td>6.47</td>
                <td>26.32</td>
                <td>56.94</td>
                <td>13.33</td>
                <td>64.10</td>
                <td>16.46</td>
                <td>33.63</td>
            </tr>
            <tr>
                <td class="left-align">Intern-VL3-8B<sup>6</sup></td>
                <td>36.17</td>
                <td>41.67</td>
                <td>4.63</td>
                <td>18.39</td>
                <td>60.25</td>
                <td>2.50</td>
                <td>49.06</td>
                <td>8.38</td>
                <td>28.54</td>
            </tr>
            <tr>
                <td class="left-align">LLaVA-1.5-7B<sup>7</sup></td>
                <td>1.54</td>
                <td>36.46</td>
                <td>5.02</td>
                <td>20.69</td>
                <td>42.86</td>
                <td>5.00</td>
                <td>38.68</td>
                <td>0.52</td>
                <td>19.45</td>
            </tr>
            <tr class="group-header">
                <td colspan="10">Depth-Encoded Vision-Language Models: Image + Depth Map + Text as Input</td>
            </tr>
            <tr>
                <td class="left-align">SpatialBot<sup>8</sup></td>
                <td>10.64</td>
                <td>46.88</td>
                <td>15.83</td>
                <td>28.74</td>
                <td>66.46</td>
                <td>5.00</td>
                <td>50.94</td>
                <td>8.90</td>
                <td>29.17</td>
            </tr>
            <tr>
                <td class="left-align">SpatialRGPT<sup>9</sup></td>
                <td>10.64</td>
                <td>36.46</td>
                <td>20.08</td>
                <td>17.24</td>
                <td>60.25</td>
                <td>15.00</td>
                <td>62.26</td>
                <td>9.95</td>
                <td>28.98</td>
            </tr>
            <tr class="highlight">
                <td class="left-align">Ours</td>
                <td class="bold">87.23</td>
                <td class="bold">47.92</td>
                <td>51.35</td>
                <td>42.53</td>
                <td class="bold">75.16</td>
                <td>40.00</td>
                <td>55.66</td>
                <td>46.07</td>
                <td>56.31</td>
            </tr>
            <tr class="highlight">
                <td class="left-align">Ours w/ MSMU-CoT</td>
                <td class="bold">87.23</td>
                <td>42.71</td>
                <td class="bold">51.74</td>
                <td class="bold">49.43</td>
                <td>73.29</td>
                <td class="bold">50.00</td>
                <td class="bold">69.81</td>
                <td class="bold">49.32</td>
                <td class="bold">59.19</td>
            </tr>
        </tbody>
    </table>

  <br>
  <br>
  
<section class="hero teaser">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Results</h2>

        <div class="content has-text-justified">
          <div class="table-container">
            <table style="width: 82%;">
              <!-- 设置表格宽度为100% -->
              <caption>Table: Comparison on NeRF On-the-go Dataset</caption>
              <thead>
                <tr>
                  <th rowspan="3">Method</th>
                  <th colspan="6">Low Occlusion</th>
                  <th colspan="6">Medium Occlusion</th>
                  <th colspan="6">High Occlusion</th>
                </tr>
                <tr>
                  <th colspan="3">Mountain</th>
                  <th colspan="3">Fountain</th>
                  <th colspan="3">Corner</th>
                  <th colspan="3">Patio</th>
                  <th colspan="3">Spot</th>
                  <th colspan="3">Patio-High</th>
                </tr>
                <tr>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>LPIPS</th>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>LPIPS</th>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>LPIPS</th>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>LPIPS</th>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>LPIPS</th>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>LPIPS</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                    <td>RobustNeRF</td>
                    <td>17.54</td>
                    <td>0.496</td>
                    <td>0.383</td>
                    <td>15.65</td>
                    <td>0.318</td>
                    <td>0.576</td>
                    <td>23.04</td>
                    <td>0.764</td>
                    <td>0.244</td>
                    <td>20.39</td>
                    <td>0.718</td>
                    <td>0.251</td>
                    <td>20.65</td>
                    <td>0.625</td>
                    <td>0.391</td>
                    <td>20.54</td>
                    <td>0.578</td>
                    <td>0.366</td>
                </tr>
                <tr>
                    <td>NeRF On-the-go</td>
                    <td>20.15</td>
                    <td>0.644</td>
                    <td>0.259</td>
                    <td>20.11</td>
                    <td>0.609</td>
                    <td>0.314</td>
                    <td>24.22</td>
                    <td>0.806</td>
                    <td>0.190</td>
                    <td>20.78</td>
                    <td>0.754</td>
                    <td>0.219</td>
                    <td>23.33</td>
                    <td>0.787</td>
                    <td>0.189</td>
                    <td>21.41</td>
                    <td>0.718</td>
                    <td>0.235</td>
                </tr>
                <tr>
                    <td>3DGS</td>
                    <td>19.40</td>
                    <td>0.638</td>
                    <td class="bold"><span class="highlight">0.213</span></td>
                    <td>19.96</td>
                    <td>0.659</td>
                    <td class="bold"><span class="highlight">0.185</span></td>
                    <td>20.90</td>
                    <td>0.713</td>
                    <td>0.241</td>
                    <td>17.48</td>
                    <td>0.704</td>
                    <td>0.199</td>
                    <td>20.77</td>
                    <td>0.693</td>
                    <td>0.316</td>
                    <td>17.29</td>
                    <td>0.604</td>
                    <td>0.363</td>
                </tr>
                <tr>
                    <td>SLS-mlp*</td>
                    <td>19.84</td>
                    <td>0.580</td>
                    <td>0.294</td>
                    <td>20.19</td>
                    <td>0.612</td>
                    <td>0.258</td>
                    <td>24.03</td>
                    <td>0.795</td>
                    <td>0.258</td>
                    <td>21.55</td>
                    <td class="bold"><span class="highlight">0.838</span></td>
                    <td class="bold"><span class="highlight">0.065</span></td>
                    <td>23.52</td>
                    <td>0.756</td>
                    <td class="bold"><span class="highlight">0.185</span></td>
                    <td>20.31</td>
                    <td>0.664</td>
                    <td>0.259</td>
                </tr>
                <tr>
                    <td>HybridGS (Ours)</td>
                    <td class="bold"><span class="highlight">21.73</span></td>
                    <td class="bold"><span class="highlight">0.693</span></td>
                    <td>0.284</td>
                    <td class="bold"><span class="highlight">21.11</span></td>
                    <td class="bold"><span class="highlight">0.674</span></td>
                    <td>0.252</td>
                    <td class="bold"><span class="highlight">25.03</span></td>
                    <td class="bold"><span class="highlight">0.847</span></td>
                    <td class="bold"><span class="highlight">0.151</span></td>
                    <td class="bold"><span class="highlight">21.98</span></td>
                    <td>0.812</td>
                    <td>0.169</td>
                    <td class="bold"><span class="highlight">24.33</span></td>
                    <td class="bold"><span class="highlight">0.794</span></td>
                    <td>0.196</td>
                    <td class="bold"><span class="highlight">21.77</span></td>
                    <td class="bold"><span class="highlight">0.741</span></td>
                    <td class="bold"><span class="highlight">0.211</span></td>
                </tr>
            </tbody>
            </table>
          </div>
        </div>

        <div class="content has-text-justified">
          <div class="table-container">
            <table style="width: 100%;">
              <caption>Table: Comparison on RobustNeRF Dataset</caption>
                <thead>
                    <tr>
                        <th rowspan="2">Method</th>
                        <th colspan="3">Statue</th>
                        <th colspan="3">Android</th>
                        <th colspan="3">Yoda</th>
                        <th colspan="3">Crab (1)</th>
                        <th colspan="3">Crab (2)</th>
                    </tr>
                    <tr>
                        <th>PSNR</th>
                        <th>SSIM</th>
                        <th>LPIPS</th>
                        <th>PSNR</th>
                        <th>SSIM</th>
                        <th>LPIPS</th>
                        <th>PSNR</th>
                        <th>SSIM</th>
                        <th>LPIPS</th>
                        <th>PSNR</th>
                        <th>SSIM</th>
                        <th>LPIPS</th>
                        <th>PSNR</th>
                        <th>SSIM</th>
                        <th>LPIPS</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>RobustNeRF</td>
                        <td>20.60</td>
                        <td>0.76</td>
                        <td>0.15</td>
                        <td>23.28</td>
                        <td>0.75</td>
                        <td>0.13</td>
                        <td>29.78</td>
                        <td>0.82</td>
                        <td>0.15</td>
                        <td>32.22</td>
                        <td>0.94</td>
                        <td>0.06</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>NeRF On-the-go</td>
                        <td>21.58</td>
                        <td>0.77</td>
                        <td>0.24</td>
                        <td>23.50</td>
                        <td>0.75</td>
                        <td>0.21</td>
                        <td>29.96</td>
                        <td>0.83</td>
                        <td>0.24</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>3DGS</td>
                        <td>21.02</td>
                        <td>0.81</td>
                        <td>0.16</td>
                        <td>23.11</td>
                        <td>0.81</td>
                        <td>0.13</td>
                        <td>26.33</td>
                        <td>0.91</td>
                        <td>0.14</td>
                        <td>31.80</td>
                        <td>0.96</td>
                        <td>0.08</td>
                        <td>29.74</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>SLS-mlp</td>
                        <td>22.54</td>
                        <td>0.84</td>
                        <td>0.13</td>
                        <td>25.05</td>
                        <td class="bold"><span class="highlight">0.85</span></td>
                        <td>0.09</td>
                        <td>33.66</td>
                        <td class="bold"><span class="highlight">0.96</span></td>
                        <td>0.10</td>
                        <td>35.85</td>
                        <td class="bold"><span class="highlight">0.97</span></td>
                        <td>0.08</td>
                        <td>34.43</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>HybridGS (Ours)</td>
                        <td class="bold"><span class="highlight">22.93</span></td>
                        <td class="bold"><span class="highlight">0.87</span></td>
                        <td class="bold"><span class="highlight">0.10</span></td>
                        <td class="bold"><span class="highlight">25.15</span></td>
                        <td class="bold"><span class="highlight">0.85</span></td>
                        <td class="bold"><span class="highlight">0.07</span></td>
                        <td class="bold"><span class="highlight">35.32</span></td>
                        <td class="bold"><span class="highlight">0.96</span></td>
                        <td class="bold"><span class="highlight">0.07</span></td>
                        <td class="bold"><span class="highlight">36.31</span></td>
                        <td class="bold"><span class="highlight">0.97</span></td>
                        <td class="bold"><span class="highlight">0.05</span></td>
                        <td class="bold"><span class="highlight">35.17</span></td>
                        <td class="bold"><span class="highlight">0.96</span></td>
                        <td class="bold"><span class="highlight">0.08</span></td>
                    </tr>
                </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<br>
<br>


<section class="hero teaser">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Visualization</h2>
        <img src="./static/images/4.jpg" />
        <figcaption>Figure: Visualization of novel view synthesis results on the testing set of NeRF On-the-go dataset.</figcaption>
        <br>
        <img src="./static/images/5.jpg" />
        <figcaption>Figure: Visualization of scene decomposition into transients and statics.</figcaption>
        <br>
        <video width="640" height="360" controls>
          <source src="./static/videos/Training_Process.mp4" type="video/mp4">
          <source src="video.webm" type="video/webm">
          <p>Your browser does not support HTML5 video. Here is a <a href="video.mp4">link to the video</a> instead.</p>
        </video>
        <figcaption>Figure: Qualitative results compared to 3DGS during the training steps.</figcaption>
        <div class="content has-text-justified">

        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Our method demonstrates superior results by effectively reducing artifacts and providing clearer boundaries. This results in a cleaner statics compared to other methods, showcasing enhanced visual quality and precision in novel views. 
          </p>
          <p>
            Also, our method achieves superior transient mask separation in both indoor and outdoor scenes. It effectively separates transients and statics, and the resulting renderings closely resemble the ground truth images, demonstrating its effectiveness.
          </p>
            Finally, as training iterations increase, 3DGS tends to gradually integrate transient elements into the static components, rendering the residuals being almost incapable of capturing transient contents. In contrast, our HybridGS effectively distinguishes transients from statics over time, leading to consistent improvements.
          <p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<br>
<br>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{lin2024hybridgs,
    author = {Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye},
    title = {HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting},
    booktitle = {CVPR},
    year = {2025}
}</code></pre>
  </div>
</section>



</body>
</html>
